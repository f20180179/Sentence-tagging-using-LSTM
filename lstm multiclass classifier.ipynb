{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script wheel.exe is installed in 'C:\\Users\\Ashutosh Sharma\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tensorflow-2.5.0-cp38-cp38-win_amd64.whl (422.6 MB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\ashutosh sharma\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.19.2)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-win_amd64.whl (2.9 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\ashutosh sharma\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-win_amd64.whl (909 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.33.1-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\program files\\python38\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (49.2.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Downloading charset_normalizer-2.0.3-py3-none-any.whl (35 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for wrapt, since package 'wheel' is not installed.\n",
      "Installing collected packages: h5py, wheel, absl-py, grpcio, google-pasta, keras-preprocessing, opt-einsum, astunparse, flatbuffers, tensorboard-data-server, werkzeug, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, charset-normalizer, urllib3, idna, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, markdown, protobuf, tensorboard, termcolor, gast, tensorflow-estimator, wrapt, typing-extensions, keras-nightly, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.3.0\n",
      "    Uninstalling h5py-3.3.0:\n",
      "      Successfully uninstalled h5py-3.3.0\n",
      "    Running setup.py install for termcolor: started\n",
      "    Running setup.py install for termcolor: finished with status 'done'\n",
      "    Running setup.py install for wrapt: started\n",
      "    Running setup.py install for wrapt: finished with status 'done'\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 certifi-2021.5.30 charset-normalizer-2.0.3 flatbuffers-1.12 gast-0.4.0 google-auth-1.33.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 idna-3.2 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.6 werkzeug-2.0.1 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\Ashutosh Sharma\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\Ashutosh Sharma\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\Ashutosh Sharma\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\Ashutosh Sharma\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Ashutosh Sharma\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\Ashutosh Sharma\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 20.2.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set()\n",
    "word_to_vec = {}\n",
    "word_to_idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.txt', 'r', encoding='utf8') as fr:\n",
    "    for line in fr:\n",
    "        line = line.strip().split()\n",
    "        curr_w = line[0]\n",
    "        word_to_vec[curr_w] = np.array(line[1:], dtype=np.float64)\n",
    "        words_set.add(curr_w)\n",
    "    i = 1\n",
    "    for word in sorted(words_set):\n",
    "        word_to_idx[word] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178126"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\"hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.54313  ,  0.34427  ,  0.27125  ,  1.0487   , -1.1642   ,\n",
       "       -1.2722   ,  0.35781  , -0.56527  , -0.29879  ,  0.85179  ,\n",
       "        0.52222  , -0.0019718, -0.46435  ,  0.033631 ,  0.048367 ,\n",
       "        0.78762  ,  0.075995 ,  0.51577  ,  0.34778  ,  0.53802  ,\n",
       "        0.28299  , -0.1313   , -0.073753 ,  0.42614  ,  0.030954 ,\n",
       "       -0.55033  , -0.99789  , -0.28947  ,  0.30517  , -1.1194   ,\n",
       "        1.2957   ,  0.91165  ,  0.32222  ,  0.93405  , -0.34152  ,\n",
       "       -0.62713  , -0.092165 ,  0.50901  ,  0.29204  , -0.20122  ,\n",
       "        0.19614  , -0.45882  ,  1.1099   , -0.68737  ,  1.5724   ,\n",
       "       -0.10446  ,  0.23594  , -0.56594  ,  0.43676  ,  0.98093  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_mapping = {\"0\": \":heart:\", \"1\": \":ball:\", \"2\": \":smile:\", \"3\": \":disappointed:\", \"4\":\":fork_and_knife:\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(X, word_to_idx, max_len):\n",
    "    num_training_samples = X.shape[0]\n",
    "    #pad each sentence with zeros to make each sequence of equal length\n",
    "    X_processed = np.zeros((num_training_samples, max_len))\n",
    "    \n",
    "    for i in range(num_training_samples):\n",
    "        words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for word in words:\n",
    "            #replace word by their index in our vocabulary, embedding of which is already known\n",
    "            X_processed[i,j] = word_to_idx[word]\n",
    "            j += 1\n",
    "    \n",
    "    return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the embedding layer(pre-trained)\n",
    "def embed_layer(word_to_vec, word_to_idx):\n",
    "    vocab_size = len(word_to_idx)+1\n",
    "    embVec_dim = word_to_vec['sample'].shape[0]\n",
    "    embed_matrix = np.zeros((vocab_size, embVec_dim))\n",
    "    for k,v in word_to_idx.items():\n",
    "        embed_matrix[v,] = word_to_vec[k]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_size, embVec_dim, trainable=False)#static embedding vectors\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([embed_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(input_shape, word_to_vec, word_to_idx):\n",
    "    input_sentences = Input(shape=input_shape, dtype='int32')\n",
    "    embedding_layer = embed_layer(word_to_vec, word_to_idx)\n",
    "    embeddings = embedding_layer(input_sentences)\n",
    "    \n",
    "    X = Bidirectional(LSTM(128, return_sequences = True))(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Bidirectional(LSTM(128, return_sequences = False))(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    model = Model(inputs = input_sentences, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "with open('train_emoji.csv') as data:\n",
    "    csv_reader = csv.reader(data)\n",
    "    for row in csv_reader:\n",
    "        X_train.append(row[0])\n",
    "        y_train.append(row[1])\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train, dtype=int)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "with open('test_emoji.csv') as data:\n",
    "    csv_reader = csv.reader(data)\n",
    "    for row in csv_reader:\n",
    "        X_test.append(row[0])\n",
    "        y_test.append(row[1])\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'never talk to me again'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_of_sentence = 0\n",
    "for i in range(len(X_train)):\n",
    "    max_len_of_sentence = max(max_len_of_sentence, len(X_train[i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(max_len_of_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model((max_len_of_sentence,), word_to_vec, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = process_sentence(X_train, word_to_idx, max_len_of_sentence)\n",
    "y_train_oh = np.eye(5)[y_train.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_processed = X_train_processed[-20:]\n",
    "y_val_oh = y_train_oh[-20:]\n",
    "X_train_processed = X_train_processed[:-20]\n",
    "y_train_oh = y_train_oh[:-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 7s 193ms/step - loss: 1.5859 - accuracy: 0.2126 - val_loss: 1.4533 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 1.3892 - accuracy: 0.4275 - val_loss: 1.2282 - val_accuracy: 0.6500\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 1.0931 - accuracy: 0.6607 - val_loss: 0.9946 - val_accuracy: 0.5500\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.7852 - accuracy: 0.7255 - val_loss: 1.0786 - val_accuracy: 0.5500\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.6613 - accuracy: 0.7524 - val_loss: 0.7091 - val_accuracy: 0.6500\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.4476 - accuracy: 0.8463 - val_loss: 0.5854 - val_accuracy: 0.7000\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.3641 - accuracy: 0.8931 - val_loss: 0.7612 - val_accuracy: 0.7000\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.3363 - accuracy: 0.8734 - val_loss: 0.8081 - val_accuracy: 0.6000\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.2833 - accuracy: 0.8957 - val_loss: 0.7922 - val_accuracy: 0.6000\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.2081 - accuracy: 0.9137 - val_loss: 0.6382 - val_accuracy: 0.6500\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.1605 - accuracy: 0.9435 - val_loss: 0.5830 - val_accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.1102 - accuracy: 0.9566 - val_loss: 0.7419 - val_accuracy: 0.7000\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.1638 - accuracy: 0.9383 - val_loss: 0.9755 - val_accuracy: 0.6500\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.2191 - accuracy: 0.9322 - val_loss: 0.7377 - val_accuracy: 0.7000\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.1804 - accuracy: 0.9302 - val_loss: 0.8618 - val_accuracy: 0.6500\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.0871 - accuracy: 0.9723 - val_loss: 0.7316 - val_accuracy: 0.7000\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0648 - accuracy: 0.9737 - val_loss: 0.6719 - val_accuracy: 0.7000\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0636 - accuracy: 0.9749 - val_loss: 0.9551 - val_accuracy: 0.7500\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0348 - accuracy: 0.9973 - val_loss: 0.7546 - val_accuracy: 0.7000\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.0276 - accuracy: 0.9983 - val_loss: 0.8513 - val_accuracy: 0.7500\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9907 - val_loss: 0.7053 - val_accuracy: 0.8000\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.5702 - val_accuracy: 0.8000\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.5290 - val_accuracy: 0.8000\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7038 - val_accuracy: 0.8000\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0159 - accuracy: 0.9925 - val_loss: 1.9938 - val_accuracy: 0.6500\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.2947 - accuracy: 0.9305 - val_loss: 1.5346 - val_accuracy: 0.7000\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.3813 - accuracy: 0.8995 - val_loss: 1.0141 - val_accuracy: 0.7500\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.2519 - accuracy: 0.9143 - val_loss: 0.6810 - val_accuracy: 0.8000\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 0.1449 - accuracy: 0.9537 - val_loss: 0.7717 - val_accuracy: 0.7000\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1037 - accuracy: 0.9652 - val_loss: 0.5233 - val_accuracy: 0.7500\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0590 - accuracy: 0.9886 - val_loss: 0.7041 - val_accuracy: 0.7500\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0491 - accuracy: 0.9925 - val_loss: 0.6549 - val_accuracy: 0.7500\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0409 - accuracy: 0.9790 - val_loss: 0.4491 - val_accuracy: 0.8000\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.4997 - val_accuracy: 0.8500\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 0.7500\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 0.8000\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.7500\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.6959 - val_accuracy: 0.7500\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5808 - val_accuracy: 0.8000\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4960 - val_accuracy: 0.8500\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5315 - val_accuracy: 0.8000\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 0.8000\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.7500\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6171 - val_accuracy: 0.8000\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5903 - val_accuracy: 0.8000\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5952 - val_accuracy: 0.8000\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5971 - val_accuracy: 0.8000\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 0.8000\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 0.8000\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 9.3234e-04 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21678f54280>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_processed, y_train_oh, epochs=50, validation_data=(X_val_processed, y_val_oh), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9755 - accuracy: 0.8036\n",
      "Loss:  0.9754543304443359  Accuracy:  0.8035714030265808\n"
     ]
    }
   ],
   "source": [
    "X_test_processed = process_sentence(X_test, word_to_idx, max_len_of_sentence)\n",
    "y_test_oh = np.eye(5)[y_test.reshape(-1)]\n",
    "loss, acc = model.evaluate(X_test_processed, y_test_oh)\n",
    "print(\"Loss: \", loss, \" Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.4.1.tar.gz (185 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.4.1-py3-none-any.whl size=186373 sha256=0fe557eae3b83601d423956d2274d2d5e473f46d716e2c1c59b01104e43b4dc1\n",
      "  Stored in directory: c:\\users\\ashutosh sharma\\appdata\\local\\pip\\cache\\wheels\\9e\\a5\\d8\\dea02b6be962ee32404006efa68513e8701aabc12fd80fd3f6\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-1.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_string):\n",
    "    x_test = np.array([input_string])\n",
    "    x_test_processed = process_sentence(x_test, word_to_idx, max_len_of_sentence)\n",
    "    result = str(np.argmax(model.predict(x_test_processed)))\n",
    "    print(x_test[0] + ' ' + emoji.emojize(emoji_mapping[result], use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not great üòû\n"
     ]
    }
   ],
   "source": [
    "test_model('This is not great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is great üòÑ\n"
     ]
    }
   ],
   "source": [
    "test_model('This is great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dinner is delicious üç¥\n"
     ]
    }
   ],
   "source": [
    "test_model('The dinner is delicious')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
